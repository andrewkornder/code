{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4d84fdcd-6808-4a52-a789-fc27985adc89",
   "metadata": {},
   "source": [
    "Results\n",
    "# n/a: 09.841%\n",
    "# 981: 95.835%\n",
    "# 825: 96.857% (more epochs [30, 10] => [100, 50])\n",
    "# 778: 97.189% (metric change from \"accuracy\" to keras.metrics.SparseCategoricalAccuracy)\n",
    "\n",
    "notes:\n",
    "- using batch_size = 5 resulted in the network overfitting and scoring a 9.896%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49064de-efaa-415f-ac17-b1f86415d446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "\n",
    "- replace each pixel value with some scaled distance to nearest activated pixel\n",
    "- maybe convert to vector graphics?\n",
    "- second model to differentiate between closely related images\n",
    "- play around w loss_fn, activation funcs, optimizers and hidden layer sizes\n",
    "- threading to train many model at once for a kinda gridsearch for batch_size and epochs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e38498-ebb0-4a9a-bb70-4a10f3102fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "y = np.asarray(train['label']).astype('int32').reshape((-1,1))\n",
    "train = train.drop('label', axis=1)\n",
    "\n",
    "answers = [a[0] for a in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c86df1-ea19-49ae-89c3-7af5a9a4e85d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, X, y, output_size, hidden_layers, activation='sigmoid', optimizer='adam',\n",
    "                 hidden_size=None, loss_fn=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics=('accuracy',)):\n",
    "        self.X, self.y = X, y\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        size = len(X.iloc[0])\n",
    "        hidden_size = hidden_size if hidden_size else 2 * size // 3 + output_size\n",
    "        \n",
    "        self.model.add(Dense(hidden_size, input_shape=(size,), activation=activation))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.model.add(Dense(hidden_size, activation=activation))\n",
    "            \n",
    "        self.model.add(Dense(output_size, activation=activation))\n",
    "        \n",
    "        self.model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "    def fit(self, epochs=30, batch_size=10, verbose=1): self.model.fit(self.X, self.y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    def score(self): return self.model.hist.history['accuracy'][-1]\n",
    "    \n",
    "    def get_errors(self): return list(zip(*[(answers[i], a) for i, a in enumerate([a.argmax() for a in self.model.predict(train)]) if a != y[i]]))\n",
    "    \n",
    "    def get_threshold(self, a, b, predictions, boolean=False):\n",
    "        error = [abs(x[a] - x[b]) for i, x in enumerate(predictions) if answers[i] in (a, b) and ((answers[i] == x.argmax()) == boolean)] \n",
    "        return sum(error) / len(error)\n",
    "    \n",
    "    def get_close(self, predictions, a, b, threshold=None):\n",
    "        threshold = threshold if threshold else self.get_threshold(a, b, predictions)\n",
    "        close = {i: train[i] for i, x in enumerate(predictions) if abs(x[a] - x[b]) < threshold}  # all inputs which were close\n",
    "        for i, data in close.items():\n",
    "            # use second model and save result, then edit predictions to have the new results\n",
    "            continue\n",
    "    \n",
    "    def display_row(i): Image.fromarray(np.reshape(np.array(X.iloc[i].values), (28, 28)).astype('uint8')).show()\n",
    "        \n",
    "    def save_output(self, data, output='predictions.csv'):\n",
    "        pd.DataFrame(columns=['ImageId', 'Label'], data=zip(range(1, len(data) + 1), [list(a).index(max(a)) for a in self.model.predict(data)])).to_csv(output, index=0)\n",
    "        ! kaggle competitions submit -c digit-recognizer -f $output -m \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8104a1f1-a590-4deb-8baf-9b28e97825a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4331 - sparse_categorical_accuracy: 0.0995\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 2.3248 - sparse_categorical_accuracy: 0.1951\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 2.3384 - sparse_categorical_accuracy: 0.1831\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 1s 592ms/step - loss: 2.2184 - sparse_categorical_accuracy: 0.2561\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 2.0977 - sparse_categorical_accuracy: 0.5057\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 2.0185 - sparse_categorical_accuracy: 0.4792\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 1.9717 - sparse_categorical_accuracy: 0.2859\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 1.9041 - sparse_categorical_accuracy: 0.2472\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 1.7842 - sparse_categorical_accuracy: 0.4577\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 1.6713 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 1.5831 - sparse_categorical_accuracy: 0.6742\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 1.5010 - sparse_categorical_accuracy: 0.6433\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 1.4112 - sparse_categorical_accuracy: 0.6474\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 1.3123 - sparse_categorical_accuracy: 0.6678\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 1.2119 - sparse_categorical_accuracy: 0.7148\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 1.1213 - sparse_categorical_accuracy: 0.7709\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 1.0471 - sparse_categorical_accuracy: 0.7930\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 0.9827 - sparse_categorical_accuracy: 0.7958\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.9159 - sparse_categorical_accuracy: 0.8075\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.8475 - sparse_categorical_accuracy: 0.8251\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 0.7871 - sparse_categorical_accuracy: 0.8332\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 1s 692ms/step - loss: 0.7377 - sparse_categorical_accuracy: 0.8348\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 1s 624ms/step - loss: 0.6939 - sparse_categorical_accuracy: 0.8387\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 0.6506 - sparse_categorical_accuracy: 0.8487\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 0.6077 - sparse_categorical_accuracy: 0.8591\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 0.5683 - sparse_categorical_accuracy: 0.8669\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 0.5349 - sparse_categorical_accuracy: 0.8735\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.5064 - sparse_categorical_accuracy: 0.8785\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.4798 - sparse_categorical_accuracy: 0.8828\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.4541 - sparse_categorical_accuracy: 0.8867\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 0.4304 - sparse_categorical_accuracy: 0.8905\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 0.4095 - sparse_categorical_accuracy: 0.8942\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 0.3913 - sparse_categorical_accuracy: 0.8970\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.3751 - sparse_categorical_accuracy: 0.9001\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.9035\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 0.3458 - sparse_categorical_accuracy: 0.9058\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.3326 - sparse_categorical_accuracy: 0.9089\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 0.3207 - sparse_categorical_accuracy: 0.9116\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 1s 619ms/step - loss: 0.3098 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.3000 - sparse_categorical_accuracy: 0.9158\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.2907 - sparse_categorical_accuracy: 0.9182\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.9206\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 1s 626ms/step - loss: 0.2735 - sparse_categorical_accuracy: 0.9226\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.2660 - sparse_categorical_accuracy: 0.9245\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.9263\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.9283\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.2459 - sparse_categorical_accuracy: 0.9302\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.2399 - sparse_categorical_accuracy: 0.9317\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.2342 - sparse_categorical_accuracy: 0.9334\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.2288 - sparse_categorical_accuracy: 0.9347\n"
     ]
    }
   ],
   "source": [
    "model = Model(train, y, 10, 3, metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(epochs=50, batch_size=420000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac2dbd-d46c-4b54-ad88-402aee5d9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_output(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
